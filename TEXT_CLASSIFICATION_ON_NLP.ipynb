{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TEXT CLASSIFICATION ON NLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLdj4O0Zn2GW",
        "outputId": "e69d6915-e9b7-454d-873b-1b9d1d2bb83a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(csv_file, split=0.9):\n",
        "    data = pd.read_csv(csv_file,nrows=100)\n",
        "    \n",
        "    # Shuffle data\n",
        "    train_data = data.sample(frac=1, random_state=7)\n",
        "    \n",
        "    texts = train_data.text.values\n",
        "    labels = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)}\n",
        "              for y in train_data.sentiment.values]\n",
        "    split = int(len(train_data) * split)\n",
        "    \n",
        "    train_labels = [{\"cats\": labels} for labels in labels[:split]]\n",
        "    val_labels = [{\"cats\": labels} for labels in labels[split:]]\n",
        "    \n",
        "    return texts[:split], train_labels, texts[split:], val_labels\n",
        "\n",
        "train_texts, train_labels, val_texts, val_labels = load_data('/content/yelp_ratings.csv')"
      ],
      "metadata": {
        "id": "BE0M5W8KxeJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Texts from training data\\n------')\n",
        "print(train_texts[:2])\n",
        "print('\\nLabels from training data\\n------')\n",
        "train_labels[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UCakMfgz5Ge",
        "outputId": "28e7a80a-aba4-4ac0-dda0-68086d0c4831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texts from training data\n",
            "------\n",
            "['I called the number provided and same day they showed up to my residence for an inspection. Was able to advise me of multiple broken and loose tiles on my roof and came out just a few days later and completed the repairs. Quoted me a good price for the repairs as well. No complaints!'\n",
            " \"ended up here because Raku was closed and it received great ratings on Yelp.  I'm so glad I came here.  One of the better meals I've had.  Started off with the mushroom dish and the lettuce wrap.  both were amazing. the lettuce wrap is like having a flavor party in your mouth.  also had the panang duck which was terrific. highly recommend all three dishes. one dish that wasn't so good was the seabass with drunken noodles. overall it was an excellent meal, intimate setting, and great service. definitely will be back.\"]\n",
            "\n",
            "Labels from training data\n",
            "------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'cats': {'NEGATIVE': False, 'POSITIVE': True}},\n",
              " {'cats': {'NEGATIVE': False, 'POSITIVE': True}}]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U spacy==2.3.6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tzu8oKE2-Lrx",
        "outputId": "721ef381-299a-4fe0-de13-48fd9a697d29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy==2.3.6 in /usr/local/lib/python3.7/dist-packages (2.3.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.6) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.6) (4.64.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.6) (1.0.5)\n",
            "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.6) (7.4.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.6) (1.21.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.6) (3.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.6) (1.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.6) (0.9.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.6) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.6) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.6) (1.0.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.6) (0.7.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.6) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.3.6) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.3.6) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.3.6) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.6) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.6) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.6) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.6) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Create an empty model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Create the TextCategorizer with exclusive classes and \"bow\" architecture\n",
        "textcat = nlp.create_pipe(\n",
        "              \"textcat\",\n",
        "              config={\n",
        "                \"exclusive_classes\": True,\n",
        "                \"architecture\": \"bow\"})\n",
        "\n",
        "# Add the TextCategorizer to the empty model\n",
        "nlp.add_pipe(textcat)\n",
        "\n",
        "# Add labels to text classifier\n",
        "textcat.add_label(\"NEGATIVE\")\n",
        "textcat.add_label(\"POSITIVE\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJl-CaHI4_UY",
        "outputId": "fbfa71a4-e00a-4ae1-b6b3-0030e86c4703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.util import minibatch\n",
        "import random\n",
        "\n",
        "def train(model, train_data, optimizer):\n",
        "    losses = {}\n",
        "    random.seed(1)\n",
        "    random.shuffle(train_data)\n",
        "    \n",
        "    batches = minibatch(train_data, size=8)\n",
        "    for batch in batches:\n",
        "        # train_data is a list of tuples [(text0, label0), (text1, label1), ...]\n",
        "        # Split batch into texts and labels\n",
        "        texts, labels = zip(*batch)\n",
        "        \n",
        "        # Update model with texts and labels\n",
        "        model.update(texts, labels, sgd=optimizer, losses=losses)\n",
        "        \n",
        "    return losses\n",
        "    "
      ],
      "metadata": {
        "id": "yRHsujfe-47r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix seed for reproducibility\n",
        "spacy.util.fix_random_seed(1)\n",
        "random.seed(1)\n",
        "\n",
        "optimizer = nlp.begin_training()\n",
        "train_data = list(zip(train_texts, train_labels))\n",
        "losses = train(nlp, train_data, optimizer)\n",
        "print(losses['textcat'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvGgaMgcFGVM",
        "outputId": "7a6bc8c8-2624-4c4b-e6dd-dbcfc1525487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.03855263814330101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This tea cup was full of holes. Do not recommend.\"\n",
        "doc = nlp(text)\n",
        "print(doc.cats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfY2MU1fF2fN",
        "outputId": "4cfeeebe-ce91-473e-84a3-c3f938453f37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'NEGATIVE': 0.4575085937976837, 'POSITIVE': 0.5424913763999939}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 4: Making Predictions"
      ],
      "metadata": {
        "id": "bulG8bhtF9fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, texts): \n",
        "    # Use the model's tokenizer to tokenize each input text\n",
        "    docs = [model.tokenizer(train_text) for train_text in texts]\n",
        "    \n",
        "    # Use textcat to get the scores for each doc\n",
        "    textcat = model.get_pipe('textcat')\n",
        "    scores, _ = textcat.predict(docs)\n",
        "    \n",
        "    # From the scores, find the class with the highest score/probability\n",
        "    predicted_class = scores.argmax(axis=1)\n",
        "    \n",
        "    return predicted_class"
      ],
      "metadata": {
        "id": "SIVLCFWBF_S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = val_texts[34:36]\n",
        "predictions = predict(nlp, texts)\n",
        "\n",
        "for p, t in zip(predictions, texts):\n",
        "    print(f\"{textcat.labels[p]}: {t} \\n\")"
      ],
      "metadata": {
        "id": "ohoWEwrOG5ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate the model"
      ],
      "metadata": {
        "id": "MxWWzIHDbA6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate(model, texts, labels):\n",
        "    \"\"\" Returns the accuracy of a TextCategorizer model. \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        model: ScaPy model with a TextCategorizer\n",
        "        texts: Text samples, from load_data function\n",
        "        labels: True labels, from load_data function\n",
        "    \n",
        "    \"\"\"\n",
        "    # Get predictions from textcat model\n",
        "    predicted_class = predict(model, texts)\n",
        "    \n",
        "    # From labels, get the true class as a list of integers (POSITIVE -> 1, NEGATIVE -> 0)\n",
        "    true_class = [int(each['cats']['POSITIVE']) for each in labels]\n",
        "    \n",
        "    # A boolean or int array indicating correct predictions\n",
        "    correct_predictions = predicted_class == true_class\n",
        "    \n",
        "    # The accuracy, number of correct predictions divided by all predictions\n",
        "    accuracy = correct_predictions.mean()\n",
        "    \n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "gZJSQBd7bEVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluate(nlp, val_texts, val_labels)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjkSfyhdbRTf",
        "outputId": "e846456c-16e1-4b58-9cdc-c567065a8973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_iters = 5\n",
        "for i in range(n_iters):\n",
        "    losses = train(nlp, train_data, optimizer)\n",
        "    accuracy = evaluate(nlp, val_texts, val_labels)\n",
        "    print(f\"Loss: {losses['textcat']:.3f} \\t Accuracy: {accuracy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wxMzDbnbZUK",
        "outputId": "80c9c9ca-ce9b-47d8-9036-f7cf9aac5697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.008 \t Accuracy: 0.900\n",
            "Loss: 0.004 \t Accuracy: 0.900\n",
            "Loss: 0.006 \t Accuracy: 1.000\n",
            "Loss: 0.002 \t Accuracy: 1.000\n",
            "Loss: 0.001 \t Accuracy: 1.000\n"
          ]
        }
      ]
    }
  ]
}